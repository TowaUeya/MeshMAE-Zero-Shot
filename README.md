# MeshMAE Zero-Shot – Fossil Mesh Clustering

Zero-shot clustering pipeline tailored for fossil and osteological meshes. The repository
wraps the official [MeshMAE](https://github.com/Maple728/MeshMAE) training utilities and
adds domain-adaptation pretraining, embedding extraction, automatic cluster selection,
and reporting.

## Repository layout

```
meshmae-zero-shot/
├── README.md
├── env/
│   └── requirements.txt         # Python dependencies (MeshMAE + clustering stack)
├── configs/
│   ├── pretrain_target.yaml     # SSL continuation on fossil domain
│   ├── extract.yaml             # Embedding extraction defaults
│   └── cluster.yaml             # Auto-k, clustering, reporting
├── datasets/
│   ├── fossils_raw/             # User-provided raw meshes (.ply/.stl)
│   └── fossils_maps/            # Processed meshes (manifold + ~500 faces + MAPS)
├── checkpoints/
│   ├── shapenet_pretrain.pkl    # Downloaded MeshMAE checkpoint (input)
│   └── fossils_target.pkl       # SSL continuation output
├── src/
│   ├── preprocess/
│   │   └── make_manifold_and_maps.py
│   ├── pretrain/
│   │   └── run_target_pretrain.sh
│   ├── embed/
│   │   └── extract_embeddings.py
│   ├── cluster/
│   │   ├── auto_k.py
│   │   ├── run_clustering.py
│   │   ├── plot_dendrogram.py
│   │   └── templates/
│   │       └── report.html
│   └── viz/
│       └── render_thumbs.py
├── out/                         # Generated artefacts (plots, csv, html)
└── embeddings/                  # Stored embeddings (npy/csv)
```

`.gitignore` keeps heavy artefacts out of version control while preserving empty directory
placeholders via `.gitkeep`.

## Environment setup

1. Create a virtual environment (Python ≥ 3.9 recommended).
2. Install dependencies:

```bash
pip install -r env/requirements.txt
```

The requirements cover MeshMAE prerequisites (PyTorch ≥ 1.11, CUDA 11.1+), mesh tooling
(`trimesh`, `open3d`, `pyvista`) and clustering/visualisation packages (`scikit-learn`,
`hdbscan`, `umap-learn`, `matplotlib`, `seaborn`).

> **Tip:** Install the official MeshMAE repository in editable mode alongside this repo:
>
> ```bash
> git clone https://github.com/Maple728/MeshMAE.git ../MeshMAE
> pip install -e ../MeshMAE
> ```

## Data expectations

- `datasets/fossils_raw/`: raw fossil meshes (.ply/.stl/.obj). Supply your own data.
- `datasets/fossils_maps/`: MeshMAE-ready meshes (~500 faces, manifold, optional MAPS
  hierarchy). Generated by the preprocessing script.
- Optional metadata (CSV) with at least a `sample_id` column to accompany embeddings.

## 1. Preprocess meshes

MeshMAE experiments assume manifold meshes with ~500 faces and MAPS subdivision levels.
Use the wrapper below to call the official preprocessing/MAPS tools. When `--make_maps`
is enabled, provide the SubdivNet `datagen_maps.py` script (or equivalent) via
`--maps_script`.

```bash
python -m src.preprocess.make_manifold_and_maps \
  --in datasets/fossils_raw \
  --out datasets/fossils_maps \
  --target_faces 500 \
  --make_maps \
  --maps_script /path/to/datagen_maps.py \
  --metadata datasets/fossils_maps/processing_metadata.json
```

Outputs remain in a mirrored directory structure under `datasets/fossils_maps/` and a
JSON manifest summarises face counts, scaling, and MAPS status.

## 2. Continue self-supervised pretraining (domain adaptation)

Place the published MeshMAE checkpoint (e.g. `shapenet_pretrain.pkl`) in `checkpoints/`.
Then launch the wrapper script which reads hyper-parameters from
`configs/pretrain_target.yaml` and forwards them to the official
`scripts/pretrain/train_pretrain.sh` entrypoint.

```bash
# optional: export MESHMAE_ROOT=../MeshMAE if the repo lives elsewhere
bash src/pretrain/run_target_pretrain.sh --config configs/pretrain_target.yaml
```

The script resolves `--dataroot`, `--batch_size`, `--epochs`, etc. from the YAML file. To
inspect without running, append `--dry-run`.

> **Advanced:** If you patched the MeshMAE pretraining code to support SSL continuation
> from an initial checkpoint, set `resume_checkpoint`/`save_checkpoint` in the YAML to
> point at `checkpoints/shapenet_pretrain.pkl` and your target output file. Otherwise the
> script executes the vanilla pretraining routine on fossils.

## 3. Extract embeddings

After (or instead of) domain adaptation, convert processed meshes to fixed-length feature
vectors.

```bash
python -m src.embed.extract_embeddings \
  --config configs/extract.yaml \
  --model-factory meshmae.models_mae.mae_vit_base_patch16 \
  --normalize
```

The extractor supports two modes:

1. **MeshMAE encoder:** requires the official package. Pass `--model-factory` pointing to
   a function that builds the encoder architecture. Checkpoint loading is controlled by
   `configs/extract.yaml` (`input.checkpoint`). The script calls
   `forward_encoder(...)`/`encode(...)` on the model and pools either the CLS token or the
   mean patch embedding (`encoder.pool_strategy`).
2. **Geometry fallback:** if MeshMAE is unavailable, `--force-geometry` (or absence of the
   package) triggers handcrafted descriptors (bounding box, volume, surface area, inertia,
   curvature). This keeps downstream clustering runnable for smoke tests.

Embeddings land in `embeddings/raw_embeddings.npy` (configurable) alongside a metadata CSV.
Optional PCA/UMAP projections are also saved when configured.

## 4. Auto-k clustering, consensus labelling, reporting

Run the full clustering pipeline which performs scaling, PCA, automatic `k` selection
(elbow, silhouette, gap statistic, GMM BIC), K-Means + HDBSCAN, unknown detection, plots,
and HTML reporting.

```bash
python -m src.cluster.run_clustering \
  --emb embeddings/raw_embeddings.npy \
  --meta embeddings/meta.csv \
  --config configs/cluster.yaml \
  --out-dir out
```

Key outputs:

- `out/cluster/kmeans_assignments.csv`
- `out/cluster/hdbscan_assignments.csv`
- `out/cluster/consensus.csv`
- `out/cluster/summary.json`
- `out/plots/*.png` (elbow, silhouette, gap, GMM BIC, UMAP)
- `out/report/index.html` (Jinja template summarising consensus metrics + plots)

Ambiguous samples are flagged when any of the following hold: HDBSCAN label `-1`, low
silhouette (`unknown_detection.silhouette_threshold`), large centre distance (quantile), or
low GMM posterior (`unknown_detection.posterior_threshold`).

## 5. Hierarchical clustering visualisation

Generate Ward/average/single/complete dendrograms for qualitative inspection.

```bash
python -m src.cluster.plot_dendrogram \
  --emb embeddings/pca_embeddings.npy \
  --meta embeddings/meta.csv \
  --method ward \
  --out out/plots/dendrogram.png
```

## 6. Mesh thumbnails (optional)

Produce quick-look thumbnails for dashboarding.

```bash
python -m src.viz.render_thumbs \
  --input datasets/fossils_maps \
  --output out/thumbs \
  --resolution 512 512
```

## Configuration overview

- `configs/pretrain_target.yaml`: training loops parameters (epochs, mask ratio, learning
  rate) plus checkpoint paths.
- `configs/extract.yaml`: dataroot, checkpoint, pooling strategy, output paths.
- `configs/cluster.yaml`: preprocessing settings (scaling, PCA, UMAP), consensus search
  range (`k_min`, `k_max`), clustering parameters (K-Means, HDBSCAN), unknown detection
  thresholds, and the HTML template location.

## FAQ

**Why keep pretraining self-supervised?**

To preserve zero-shot behaviour. Using labels for fine-tuning can collapse novel-class
sensitivity; continuation SSL adapts statistics without supervision.

**Can I evaluate with labels?**

Yes—only for external metrics (ARI, AMI, NMI, V-measure) or linear probes. Do not leak
labels into clustering, k-selection, or training loops.

**Do I have to generate MAPS?**

MeshMAE experiments assume it. The wrapper simply orchestrates calls; refer to SubdivNet's
MAPS tooling (`datagen_maps.py`) for actual generation.

**UMAP plot missing?**

Install `umap-learn`. The plot step is skipped gracefully when the package is absent.

## Repro checklist

1. Install dependencies (`pip install -r env/requirements.txt`).
2. Download MeshMAE weights into `checkpoints/`.
3. Preprocess raw meshes (`python -m src.preprocess.make_manifold_and_maps ...`).
4. Continue SSL pretraining (`bash src/pretrain/run_target_pretrain.sh`).
5. Extract embeddings (`python -m src.embed.extract_embeddings ...`).
6. Cluster + report (`python -m src.cluster.run_clustering ...`).
7. Optional visualisations: dendrogram, thumbnails, evaluation metrics.

## Licensing & attribution

- MeshMAE: follow upstream license.
- MAPS generation tooling: SubdivNet resources (refer to their license).
- This scaffolding: MIT (customise as needed).

